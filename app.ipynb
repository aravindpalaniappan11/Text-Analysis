{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aravindp/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aravindp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP123.txt\n",
      "File already exists for URL_ID 123.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP321.txt\n",
      "File already exists for URL_ID 321.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP2345.txt\n",
      "File already exists for URL_ID 2345.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP4321.txt\n",
      "File already exists for URL_ID 4321.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP432.txt\n",
      "File already exists for URL_ID 432.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP2893.txt\n",
      "File already exists for URL_ID 2893.8. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP3355.txt\n",
      "File already exists for URL_ID 3355.6. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP3817.txt\n",
      "File already exists for URL_ID 3817.4. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP4279.txt\n",
      "File already exists for URL_ID 4279.2. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP4741.txt\n",
      "File already exists for URL_ID 4741.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP5202.txt\n",
      "File already exists for URL_ID 5202.8. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP5664.txt\n",
      "File already exists for URL_ID 5664.6. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP6126.txt\n",
      "File already exists for URL_ID 6126.4. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP6588.txt\n",
      "File already exists for URL_ID 6588.2. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP7050.txt\n",
      "File already exists for URL_ID 7050.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP7511.txt\n",
      "File already exists for URL_ID 7511.8. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP7973.txt\n",
      "File already exists for URL_ID 7973.6. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP8435.txt\n",
      "File already exists for URL_ID 8435.4. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP8897.txt\n",
      "File already exists for URL_ID 8897.2. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP9359.txt\n",
      "File already exists for URL_ID 9359.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP9820.txt\n",
      "File already exists for URL_ID 9820.8. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP10282.txt\n",
      "File already exists for URL_ID 10282.6. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP10744.txt\n",
      "File already exists for URL_ID 10744.4. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP11206.txt\n",
      "File already exists for URL_ID 11206.2. Skipping...\n",
      "Error getting title for URL_ID 11668.0: 'NoneType' object has no attribute 'get_text'\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP12129.txt\n",
      "File already exists for URL_ID 12129.8. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP12591.txt\n",
      "File already exists for URL_ID 12591.6. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP13053.txt\n",
      "File already exists for URL_ID 13053.4. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP13515.txt\n",
      "File already exists for URL_ID 13515.2. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP13977.txt\n",
      "File already exists for URL_ID 13977.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP14438.txt\n",
      "File already exists for URL_ID 14438.8. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP14900.txt\n",
      "File already exists for URL_ID 14900.6. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP15362.txt\n",
      "File already exists for URL_ID 15362.4. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP15824.txt\n",
      "File already exists for URL_ID 15824.2. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP16286.txt\n",
      "File already exists for URL_ID 16286.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP16747.txt\n",
      "File already exists for URL_ID 16747.8. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP17209.txt\n",
      "File already exists for URL_ID 17209.6. Skipping...\n",
      "Error getting title for URL_ID 17671.4: 'NoneType' object has no attribute 'get_text'\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP18133.txt\n",
      "File already exists for URL_ID 18133.2. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP18595.txt\n",
      "File already exists for URL_ID 18595.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP19056.txt\n",
      "File already exists for URL_ID 19056.8. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP19518.txt\n",
      "File already exists for URL_ID 19518.6. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP19980.txt\n",
      "File already exists for URL_ID 19980.4. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP20442.txt\n",
      "File already exists for URL_ID 20442.2. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP20904.txt\n",
      "File already exists for URL_ID 20904.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP21365.txt\n",
      "File already exists for URL_ID 21365.8. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP21827.txt\n",
      "File already exists for URL_ID 21827.6. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP22289.txt\n",
      "File already exists for URL_ID 22289.4. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP22751.txt\n",
      "File already exists for URL_ID 22751.2. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP23213.txt\n",
      "File already exists for URL_ID 23213.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP23674.txt\n",
      "File already exists for URL_ID 23674.8. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP24136.txt\n",
      "File already exists for URL_ID 24136.6. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP24598.txt\n",
      "File already exists for URL_ID 24598.4. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP25060.txt\n",
      "File already exists for URL_ID 25060.2. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP25522.txt\n",
      "File already exists for URL_ID 25522.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP25983.txt\n",
      "File already exists for URL_ID 25983.8. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP26445.txt\n",
      "File already exists for URL_ID 26445.6. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP26907.txt\n",
      "File already exists for URL_ID 26907.4. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP27369.txt\n",
      "File already exists for URL_ID 27369.2. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP27831.txt\n",
      "File already exists for URL_ID 27831.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP28292.txt\n",
      "File already exists for URL_ID 28292.8. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP28754.txt\n",
      "File already exists for URL_ID 28754.6. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP29216.txt\n",
      "File already exists for URL_ID 29216.4. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP29678.txt\n",
      "File already exists for URL_ID 29678.2. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP30140.txt\n",
      "File already exists for URL_ID 30140.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP30601.txt\n",
      "File already exists for URL_ID 30601.8. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP31063.txt\n",
      "File already exists for URL_ID 31063.6. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP31525.txt\n",
      "File already exists for URL_ID 31525.4. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP31987.txt\n",
      "File already exists for URL_ID 31987.2. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP32449.txt\n",
      "File already exists for URL_ID 32449.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP32910.txt\n",
      "File already exists for URL_ID 32910.8. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP33372.txt\n",
      "File already exists for URL_ID 33372.6. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP33834.txt\n",
      "File already exists for URL_ID 33834.4. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP34296.txt\n",
      "File already exists for URL_ID 34296.2. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP34758.txt\n",
      "File already exists for URL_ID 34758.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP35219.txt\n",
      "File already exists for URL_ID 35219.8. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP35681.txt\n",
      "File already exists for URL_ID 35681.6. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP36143.txt\n",
      "File already exists for URL_ID 36143.4. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP36605.txt\n",
      "File already exists for URL_ID 36605.2. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP37067.txt\n",
      "File already exists for URL_ID 37067.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP37528.txt\n",
      "File already exists for URL_ID 37528.8. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP37990.txt\n",
      "File already exists for URL_ID 37990.6. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP38452.txt\n",
      "File already exists for URL_ID 38452.4. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP38914.txt\n",
      "File already exists for URL_ID 38914.2. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP39376.txt\n",
      "File already exists for URL_ID 39376.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP39837.txt\n",
      "File already exists for URL_ID 39837.8. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP40299.txt\n",
      "File already exists for URL_ID 40299.6. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP40761.txt\n",
      "File already exists for URL_ID 40761.4. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP41223.txt\n",
      "File already exists for URL_ID 41223.2. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP41685.txt\n",
      "File already exists for URL_ID 41685.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP42146.txt\n",
      "File already exists for URL_ID 42146.8. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP42608.txt\n",
      "File already exists for URL_ID 42608.6. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP43070.txt\n",
      "File already exists for URL_ID 43070.4. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP43532.txt\n",
      "File already exists for URL_ID 43532.2. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP43994.txt\n",
      "File already exists for URL_ID 43994.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP44455.txt\n",
      "File already exists for URL_ID 44455.8. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP44917.txt\n",
      "File already exists for URL_ID 44917.6. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP45379.txt\n",
      "File already exists for URL_ID 45379.4. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP45841.txt\n",
      "File already exists for URL_ID 45841.2. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP46303.txt\n",
      "File already exists for URL_ID 46303.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP46764.txt\n",
      "File already exists for URL_ID 46764.8. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP47226.txt\n",
      "File already exists for URL_ID 47226.6. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP47688.txt\n",
      "File already exists for URL_ID 47688.4. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP48150.txt\n",
      "File already exists for URL_ID 48150.2. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP48612.txt\n",
      "File already exists for URL_ID 48612.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP49073.txt\n",
      "File already exists for URL_ID 49073.8. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP49535.txt\n",
      "File already exists for URL_ID 49535.6. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP49997.txt\n",
      "File already exists for URL_ID 49997.4. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP50459.txt\n",
      "File already exists for URL_ID 50459.2. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP50921.txt\n",
      "File already exists for URL_ID 50921.0. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP51382.txt\n",
      "File already exists for URL_ID 51382.8. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP51844.txt\n",
      "File already exists for URL_ID 51844.6. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP52306.txt\n",
      "File already exists for URL_ID 52306.4. Skipping...\n",
      "File name: /Users/aravindp/Downloads/Intern/20211030 Test Assignment/Text_Files/Data_Extraction_and_NLP52768.txt\n",
      "File already exists for URL_ID 52768.2. Skipping...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Exception handling function\n",
    "def handle_exception(url_id, action, exception):\n",
    "    print(f\"Error {action} for URL_ID {url_id}: {str(exception)}\")\n",
    "\n",
    "# Function to make a request to a URL and create a BeautifulSoup object\n",
    "def get_soup(url, url_id):\n",
    "    header = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"}\n",
    "    try:\n",
    "        response = requests.get(url, headers=header)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "    except Exception as e:\n",
    "        handle_exception(url_id, \"making request\", e)\n",
    "        return None\n",
    "\n",
    "# Function to write title and text to a file\n",
    "def write_to_file(file_name, title, article):\n",
    "    with open(file_name, 'w') as file:\n",
    "        file.write(title + '\\n' + article)\n",
    "\n",
    "# Read the URL file into the pandas object\n",
    "df = pd.read_excel('Input.xlsx')\n",
    "\n",
    "# Loop through each row in the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    url_id = row['URL_ID']\n",
    "\n",
    "    # Make a request to URL and create a BeautifulSoup object\n",
    "    soup = get_soup(url, url_id)\n",
    "    if soup is None:\n",
    "        continue\n",
    "\n",
    "    # Find title\n",
    "    try:\n",
    "        title = soup.find('h1').get_text()\n",
    "    except Exception as e:\n",
    "        handle_exception(url_id, \"getting title\", e)\n",
    "        continue\n",
    "\n",
    "    # Find text\n",
    "    article = \"\"\n",
    "    try:\n",
    "        for p in soup.find_all('p'):\n",
    "            article += p.get_text()\n",
    "    except Exception as e:\n",
    "        handle_exception(url_id, \"getting text\", e)\n",
    "\n",
    "    # Write title and text to the file if the file doesn't exist\n",
    "    file_name = f'Text_Files/Data_Extraction_and_NLP{int(url_id)}.txt'\n",
    "    full_path = os.path.join(os.getcwd(), file_name)  # Get the full path\n",
    "    print(f\"File name: {full_path}\")  # Print the full path for debugging\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(full_path):\n",
    "        write_to_file(full_path, title, article)\n",
    "    else:\n",
    "        print(f\"File already exists for URL_ID {url_id}. Skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "text_dir = \"Text_Files\"\n",
    "stopwords_dir = \"StopWords\"\n",
    "sentiment_dir = \"MasterDictionary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length mismatch in POSITIVE SCORE: Expected 111 elements, but got 112.\n",
      "Length mismatch in NEGATIVE SCORE: Expected 111 elements, but got 112.\n",
      "Length mismatch in POLARITY SCORE: Expected 111 elements, but got 112.\n",
      "Length mismatch in SUBJECTIVITY SCORE: Expected 111 elements, but got 112.\n",
      "Length mismatch in AVG SENTENCE LENGTH: Expected 111 elements, but got 112.\n",
      "Length mismatch in PERCENTAGE OF COMPLEX WORDS: Expected 111 elements, but got 112.\n",
      "Length mismatch in FOG INDEX: Expected 111 elements, but got 112.\n",
      "Length mismatch in AVG NUMBER OF WORDS PER SENTENCE: Expected 111 elements, but got 112.\n",
      "Length mismatch in COMPLEX WORD COUNT: Expected 111 elements, but got 112.\n",
      "Length mismatch in WORD COUNT: Expected 111 elements, but got 112.\n",
      "Length mismatch in SYLLABLE PER WORD: Expected 111 elements, but got 112.\n",
      "Length mismatch in PERSONAL PRONOUNS: Expected 111 elements, but got 112.\n"
     ]
    }
   ],
   "source": [
    "# Load all stop words from the stopwords directory and store in the set variable\n",
    "stop_words = set()\n",
    "for filename in os.listdir(stopwords_dir):\n",
    "    with open(os.path.join(stopwords_dir, filename), 'r', encoding='ISO-8859-1') as f:\n",
    "        stop_words.update(set(f.read().splitlines()))\n",
    "\n",
    "# Load all text files from the directory and store in a list(docs)\n",
    "docs = []\n",
    "for text_file in os.listdir(text_dir):\n",
    "    with open(os.path.join(text_dir, text_file), 'r') as f:\n",
    "        text = f.read()\n",
    "        # Tokenize the given text file\n",
    "        words = word_tokenize(text)\n",
    "        # Remove the stop words from the tokens\n",
    "        filtered_text = [word for word in words if word.lower() not in stop_words]\n",
    "        # Add each filtered tokens of each file into a list\n",
    "        docs.append(filtered_text)\n",
    "\n",
    "# Store positive and negative words from the directory\n",
    "pos = set()\n",
    "neg = set()\n",
    "\n",
    "for filename in os.listdir(sentiment_dir):\n",
    "    if filename == 'positive-words.txt':\n",
    "        with open(os.path.join(sentiment_dir, filename), 'r', encoding='ISO-8859-1') as f:\n",
    "            pos.update(f.read().splitlines())\n",
    "    else:\n",
    "        with open(os.path.join(sentiment_dir, filename), 'r', encoding='ISO-8859-1') as f:\n",
    "            neg.update(f.read().splitlines())\n",
    "\n",
    "# Now collect the positive and negative words from each file\n",
    "# Calculate the scores from the positive and negative words\n",
    "positive_words = []\n",
    "negative_words = []\n",
    "positive_score = []\n",
    "negative_score = []\n",
    "polarity_score = []\n",
    "subjectivity_score = []\n",
    "\n",
    "# Iterate through the list of docs\n",
    "for i in range(len(docs)):\n",
    "    positive_words.append([word for word in docs[i] if word.lower() in pos])\n",
    "    negative_words.append([word for word in docs[i] if word.lower() in neg])\n",
    "    positive_score.append(len(positive_words[i]))\n",
    "    negative_score.append(len(negative_words[i]))\n",
    "    polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))\n",
    "    subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(docs[i])) + 0.000001))\n",
    "\n",
    "# Average Sentence Length, Percentage of Complex Words, Fog Index\n",
    "avg_sentence_length = []\n",
    "percentage_of_complex_words = []\n",
    "fog_index = []\n",
    "avg_syllable_word_count = []\n",
    "complex_word_count = []\n",
    "\n",
    "# Function to measure various text metrics\n",
    "def measure(file):\n",
    "    with open(os.path.join(text_dir, file), 'r') as f:\n",
    "        text = f.read()\n",
    "        # Remove punctuations\n",
    "        text = re.sub(r'[^\\w\\s.]', '', text)\n",
    "        # Split the given text file into sentences\n",
    "        sentences = text.split('.')\n",
    "        # Total number of sentences in a file\n",
    "        num_sentences = len(sentences)\n",
    "        # Total words in the file\n",
    "        words = [word for word in text.split() if word.lower() not in stop_words]\n",
    "        num_words = len(words)\n",
    "\n",
    "        # Complex words having syllable count greater than 2\n",
    "        complex_words = [word for word in words if sum(1 for letter in word if letter.lower() in 'aeiou') > 2]\n",
    "\n",
    "        # Syllable Count Per Word\n",
    "        syllable_count = sum(sum(1 for letter in word if letter.lower() in 'aeiou') for word in words)\n",
    "        syllable_words = [word for word in words if sum(1 for letter in word if letter.lower() in 'aeiou') >= 1]\n",
    "\n",
    "        avg_sentence_len = num_words / num_sentences\n",
    "        avg_syllable_word_count = syllable_count / len(syllable_words) if len(syllable_words) > 0 else 0\n",
    "        percent_complex_words = len(complex_words) / num_words\n",
    "        fog_index = 0.4 * (avg_sentence_len + percent_complex_words)\n",
    "\n",
    "        return avg_sentence_len, percent_complex_words, fog_index, avg_syllable_word_count, len(complex_words)\n",
    "\n",
    "# Iterate through each file or doc\n",
    "for file in os.listdir(text_dir):\n",
    "    x, y, z, a, b = measure(file)\n",
    "    avg_sentence_length.append(x)\n",
    "    percentage_of_complex_words.append(y)\n",
    "    fog_index.append(z)\n",
    "    avg_syllable_word_count.append(a)\n",
    "    complex_word_count.append(b)\n",
    "\n",
    "word_count = []\n",
    "average_word_length = []\n",
    "\n",
    "for file in os.listdir(text_dir):\n",
    "    with open(os.path.join(text_dir, file), 'r') as f:\n",
    "        text = re.sub(r'[^\\w\\s]', '', f.read())\n",
    "        words = [word for word in text.split() if word.lower() not in stop_words]\n",
    "        word_count.append(len(words))\n",
    "        # Calculate average word length inside the loop\n",
    "        average_length = sum(len(word) for word in words) / (len(words) + 0.000001) if len(words) > 0 else 0\n",
    "        average_word_length.append(average_length)\n",
    "\n",
    "# To calculate Personal Pronouns mentioned in the text\n",
    "def count_personal_pronouns(file):\n",
    "    with open(os.path.join(text_dir, file), 'r') as f:\n",
    "        text = f.read()\n",
    "        personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
    "        count = sum(len(re.findall(rf'\\b{pronoun}\\b', text)) for pronoun in personal_pronouns)\n",
    "    return count\n",
    "\n",
    "pp_count = [count_personal_pronouns(file) for file in os.listdir(text_dir)]\n",
    "\n",
    "# Read the output data structure\n",
    "output_df = pd.read_excel('Output Data Structure.xlsx')\n",
    "\n",
    "# URL_ID 44, 57, 144 do not exist (i.e., pages do not exist, throw 404 error)\n",
    "# so we are going to drop these rows from the table\n",
    "output_df.drop([44 - 37, 57 - 37, 144 - 37], axis=0, inplace=True)\n",
    "\n",
    "# These are the required parameters\n",
    "variables = [positive_score,\n",
    "              negative_score,\n",
    "              polarity_score,\n",
    "              subjectivity_score,\n",
    "              avg_sentence_length,\n",
    "              percentage_of_complex_words,\n",
    "              fog_index,\n",
    "              avg_syllable_word_count,\n",
    "              complex_word_count,\n",
    "              word_count,\n",
    "              pp_count,\n",
    "              average_word_length]  # Include average_word_length in the variables\n",
    "\n",
    "# Ensure all lists have the same length as the DataFrame\n",
    "expected_length = len(output_df)\n",
    "for i, var in enumerate(variables):\n",
    "    if len(var) != expected_length:\n",
    "        print(f\"Length mismatch in {output_df.columns[i + 2]}: Expected {expected_length} elements, but got {len(var)}.\")\n",
    "        # If possible, fix the length of the list to match the DataFrame length\n",
    "        # Example: var = var[:expected_length]\n",
    "\n",
    "# Write the values to the DataFrame\n",
    "for i, var in enumerate(variables):\n",
    "    output_df.iloc[:, i + 2] = var[:expected_length]  # Truncate the list if needed\n",
    "\n",
    "# Now save the DataFrame to the disk\n",
    "output_df.to_csv('Output_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aravindp/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aravindp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting title for URL_ID 11668.0: 'NoneType' object has no attribute 'get_text'\n",
      "Error getting title for URL_ID 17671.4: 'NoneType' object has no attribute 'get_text'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Exception handling function\n",
    "def handle_exception(url_id, action, exception):\n",
    "    print(f\"Error {action} for URL_ID {url_id}: {str(exception)}\")\n",
    "\n",
    "# Function to make a request to a URL and create a BeautifulSoup object\n",
    "def get_soup(url, url_id):\n",
    "    header = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"}\n",
    "    try:\n",
    "        response = requests.get(url, headers=header)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "    except Exception as e:\n",
    "        handle_exception(url_id, \"making request\", e)\n",
    "        return None\n",
    "\n",
    "# Function to write title and text to a file\n",
    "def write_to_file(file_name, title, article):\n",
    "    with open(file_name, 'w') as file:\n",
    "        file.write(title + '\\n' + article)\n",
    "\n",
    "# Read the URL file into the pandas object\n",
    "df = pd.read_excel('Input.xlsx')\n",
    "\n",
    "# Loop through each row in the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    url_id = row['URL_ID']\n",
    "\n",
    "    # Make a request to URL and create a BeautifulSoup object\n",
    "    soup = get_soup(url, url_id)\n",
    "    if soup is None:\n",
    "        continue\n",
    "\n",
    "    # Find title\n",
    "    try:\n",
    "        title = soup.find('h1').get_text()\n",
    "    except Exception as e:\n",
    "        handle_exception(url_id, \"getting title\", e)\n",
    "        continue\n",
    "\n",
    "    # Find text\n",
    "    article = \"\"\n",
    "    try:\n",
    "        for p in soup.find_all('p'):\n",
    "            article += p.get_text()\n",
    "    except Exception as e:\n",
    "        handle_exception(url_id, \"getting text\", e)\n",
    "\n",
    "    # Write title and text to the file\n",
    "    file_name = f'Text_Files/Data_Extraction_and_NLP{int(url_id)}.txt'\n",
    "    full_path = os.path.join(os.getcwd(), file_name)  # Get the full path\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
    "\n",
    "    write_to_file(full_path, title, article)\n",
    "\n",
    "# Directories\n",
    "text_dir = \"Text_Files\"\n",
    "stopwords_dir = \"StopWords\"\n",
    "sentiment_dir = \"MasterDictionary\"\n",
    "\n",
    "# Load all stop words from the stopwords directory and store in the set variable\n",
    "stop_words = set()\n",
    "for filename in os.listdir(stopwords_dir):\n",
    "    with open(os.path.join(stopwords_dir, filename), 'r', encoding='ISO-8859-1') as f:\n",
    "        stop_words.update(set(f.read().splitlines()))\n",
    "\n",
    "# Load all text files from the directory and store in a list(docs)\n",
    "docs = []\n",
    "for text_file in os.listdir(text_dir):\n",
    "    with open(os.path.join(text_dir, text_file), 'r') as f:\n",
    "        text = f.read()\n",
    "        # Tokenize the given text file\n",
    "        words = word_tokenize(text)\n",
    "        # Remove the stop words from the tokens\n",
    "        filtered_text = [word for word in words if word.lower() not in stop_words]\n",
    "        # Add each filtered token of each file into a list\n",
    "        docs.append(filtered_text)\n",
    "\n",
    "# Store positive and negative words from the directory\n",
    "pos = set()\n",
    "neg = set()\n",
    "\n",
    "for filename in os.listdir(sentiment_dir):\n",
    "    if filename == 'positive-words.txt':\n",
    "        with open(os.path.join(sentiment_dir, filename), 'r', encoding='ISO-8859-1') as f:\n",
    "            pos.update(f.read().splitlines())\n",
    "    else:\n",
    "        with open(os.path.join(sentiment_dir, filename), 'r', encoding='ISO-8859-1') as f:\n",
    "            neg.update(f.read().splitlines())\n",
    "\n",
    "# Now collect the positive and negative words from each file\n",
    "# Calculate the scores from the positive and negative words\n",
    "positive_words = []\n",
    "negative_words = []\n",
    "positive_score = []\n",
    "negative_score = []\n",
    "polarity_score = []\n",
    "subjectivity_score = []\n",
    "\n",
    "# Iterate through the list of docs\n",
    "for i in range(len(docs)):\n",
    "    positive_words.append([word for word in docs[i] if word.lower() in pos])\n",
    "    negative_words.append([word for word in docs[i] if word.lower() in neg])\n",
    "    positive_score.append(len(positive_words[i]))\n",
    "    negative_score.append(len(negative_words[i]))\n",
    "    polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))\n",
    "    subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(docs[i])) + 0.000001))\n",
    "\n",
    "# Average Sentence Length, Percentage of Complex Words, Fog Index\n",
    "avg_sentence_length = []\n",
    "percentage_of_complex_words = []\n",
    "fog_index = []\n",
    "avg_syllable_word_count = []\n",
    "complex_word_count = []\n",
    "\n",
    "# Function to measure various text metrics\n",
    "def measure(file):\n",
    "    with open(os.path.join(text_dir, file), 'r') as f:\n",
    "        text = f.read()\n",
    "        # Remove punctuations\n",
    "        text = re.sub(r'[^\\w\\s.]', '', text)\n",
    "        # Split the given text file into sentences\n",
    "        sentences = text.split('.')\n",
    "        # Total number of sentences in a file\n",
    "        num_sentences = len(sentences)\n",
    "        # Total words in the file\n",
    "        words = [word for word in text.split() if word.lower() not in stop_words]\n",
    "        num_words = len(words)\n",
    "\n",
    "        # Complex words having syllable count greater than 2\n",
    "        complex_words = [word for word in words if sum(1 for letter in word if letter.lower() in 'aeiou') > 2]\n",
    "\n",
    "        # Syllable Count Per Word\n",
    "        syllable_count = sum(sum(1 for letter in word if letter.lower() in 'aeiou') for word in words)\n",
    "        syllable_words = [word for word in words if sum(1 for letter in word if letter.lower() in 'aeiou') >= 1]\n",
    "\n",
    "        avg_sentence_len = num_words / num_sentences\n",
    "        avg_syllable_word_count = syllable_count / len(syllable_words) if len(syllable_words) > 0 else 0\n",
    "        percent_complex_words = len(complex_words) / num_words\n",
    "        fog_index = 0.4 * (avg_sentence_len + percent_complex_words)\n",
    "\n",
    "        return avg_sentence_len, percent_complex_words, fog_index, avg_syllable_word_count, len(complex_words)\n",
    "\n",
    "# Iterate through each file or doc\n",
    "for file in os.listdir(text_dir):\n",
    "    x, y, z, a, b = measure(file)\n",
    "    avg_sentence_length.append(x)\n",
    "    percentage_of_complex_words.append(y)\n",
    "    fog_index.append(z)\n",
    "    avg_syllable_word_count.append(a)\n",
    "    complex_word_count.append(b)\n",
    "\n",
    "# Word Count and Average Word Length\n",
    "word_count = []\n",
    "average_word_length = []\n",
    "\n",
    "for file in os.listdir(text_dir):\n",
    "    with open(os.path.join(text_dir, file), 'r') as f:\n",
    "        text = re.sub(r'[^\\w\\s]', '', f.read())\n",
    "        words = [word for word in text.split() if word.lower() not in stop_words]\n",
    "        length = sum(len(word) for word in words)\n",
    "        word_count.append(len(words))\n",
    "        \n",
    "        # Calculate average word length inside the loop\n",
    "        average_length = length / len(words) if len(words) > 0 else 0\n",
    "        average_word_length.append(average_length)\n",
    "\n",
    "# To calculate Personal Pronouns mentioned in the text\n",
    "def count_personal_pronouns(file):\n",
    "    with open(os.path.join(text_dir, file), 'r') as f:\n",
    "        text = f.read()\n",
    "        personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
    "        count = sum(len(re.findall(rf'\\b{pronoun}\\b', text)) for pronoun in personal_pronouns)\n",
    "    return count\n",
    "\n",
    "pp_count = [count_personal_pronouns(file) for file in os.listdir(text_dir)]\n",
    "\n",
    "# Read the output data structure\n",
    "output_df = pd.read_excel('Output Data Structure.xlsx')\n",
    "\n",
    "# URL_ID 44, 57, 144 do not exist (i.e., pages do not exist, throw 404 error)\n",
    "# so we are going to drop these rows from the table\n",
    "output_df.drop([44 - 37, 57 - 37, 144 - 37], axis=0, inplace=True)\n",
    "\n",
    "# These are the required parameters\n",
    "variables = [positive_score,\n",
    "              negative_score,\n",
    "              polarity_score,\n",
    "              subjectivity_score,\n",
    "              avg_sentence_length,\n",
    "              percentage_of_complex_words,\n",
    "              fog_index,\n",
    "              avg_syllable_word_count,\n",
    "              complex_word_count,\n",
    "              word_count,\n",
    "              pp_count,\n",
    "              average_word_length]  # Use the corrected average_word_length list\n",
    "\n",
    "# Ensure all lists have the same length as the DataFrame\n",
    "expected_length = len(output_df)\n",
    "for i, var in enumerate(variables):\n",
    "    if len(var) != expected_length:\n",
    "        print(f\"Length mismatch in {output_df.columns[i + 2]}: Expected {expected_length} elements, but got {len(var)}.\")\n",
    "        # If possible, fix the length of the list to match the DataFrame length\n",
    "        # Example: var = var[:expected_length]\n",
    "\n",
    "# Write the values to the DataFrame\n",
    "for i, var in enumerate(variables):\n",
    "    output_df.iloc[:, i + 2] = var[:expected_length]  # Truncate the list if needed\n",
    "\n",
    "# Now save the DataFrame to the disk\n",
    "output_df.to_csv('Output_Data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
